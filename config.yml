# Training Configuration
training:
  n_epochs: 10
  batch_size: 32
  lr: 1e-3
  warmup: 4000

# Model Architecture
model:
  seq_len: 350
  d_model: 512
  d_ff: 2048  # 4 * d_model
  num_heads: 8
  vocab_size: 10000
  num_layers: 6

# Data
data:
  test_size: 0.1
  tokenizer_path: null
